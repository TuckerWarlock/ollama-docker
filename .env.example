# Ollama Configuration
# Copy this file to .env to customize your setup

# Default model to use
# Options: llama3.2:1b, llama3.2:3b, llama3.2:11b, codellama:3b, codellama:7b,
#          mistral:3b, mistral:7b, qwen2.5-coder:3b, qwen2.5-coder:7b,
#          gemma3:1b, gemma3:4b
OLLAMA_MODEL=llama3.2:1b

# Ollama API host (usually not needed to change)
OLLAMA_HOST=http://localhost:11434
